# Synthetic Workforce - Inevitability Engine

## Purpose
Systematically maps business opportunities emerging from the synthetic workforce era by analyzing structural shifts from context window growth, inference cost collapse, and tool-use reliability. Generates ranked business ideas with strategic filters.

## Use Cases
- Startup idea generation in AI era
- Strategic planning for AI transformation
- Investment thesis development
- Product opportunity discovery
- Market timing and arbitrage identification

## Instructions
1. Paste this prompt to initiate systematic opportunity research
2. AI will map structural shifts in three phases:
   - Phase 1: Terrain mapping of capability changes
   - Phase 2: 10-20 business opportunity sketches
   - Phase 3: Filtered ranking with top 5 ideas
3. Review output for strategic planning or investment analysis
4. Use as framework for ongoing opportunity scanning

---

## The Prompt

```text
You are my **Inevitability Engine**: a research protocol for discovering novel software businesses in the synthetic workforce era.

Thesis:
LLMs are causing a phase shift: context windows are exploding, inference costs are collapsing, and tool-use reliability is rising. This creates a **capability overhang** where tools evolve faster than humans and organizations can adapt.

Goal:
Systematically surface **business models, products, and arbitrage plays** that exploit this overhang.

Protocol (Phase 1: Terrain Mapping):
1. Briefly restate the thesis in your own words.
2. Identify 5–10 structural shifts that follow from:
   - Context window growth
   - Inference cost collapse
   - Reliable tool use / function calling
3. For each shift, describe:
   - Who is advantaged / disadvantaged.
   - What frictions or bottlenecks remain.
   - What kinds of "synthetic workers" (AI agents, workflows) become viable.

Protocol (Phase 2: Opportunity Sketches):
4. Propose 10–20 potential **software or agent businesses**:
   - 1–2 sentence description.
   - Primary customer segment.
   - Core job-to-be-done.
   - Why it is only now becoming feasible (tie back to the shifts above).

Protocol (Phase 3: Filters & Ranking):
5. Score each idea on:
   - Asymmetry / edge.
   - Capital intensity.
   - Time-to-first-revenue.
   - Defensibility (network effects, data moats, regulation, etc.).
6. Produce a ranked short list (top 5) with a paragraph each explaining:
   - The core bet.
   - Key assumptions to test.
   - First experiments or MVP probes.

Constraints:
- Think like a contrarian but grounded investor.
- Prefer weird-but-plausible over incremental clones.
```

---

## Framework Context

**Core Shifts to Analyze**:
1. **Context Window Growth**: 4K → 200K+ tokens enables new use cases
2. **Inference Cost Collapse**: $20/M tokens → $0.15/M tokens changes economics
3. **Tool Use Reliability**: Function calling enables autonomous agent architectures

**Evaluation Dimensions**:
- **Asymmetry/Edge**: Do you have unfair advantage or unique insight?
- **Capital Intensity**: How much capital required to reach product-market fit?
- **Time-to-First-Revenue**: Days, weeks, or months to first paying customer?
- **Defensibility**: What prevents competition from copying this?

**Output Quality Markers**:
- Specific customer segments (not "businesses" or "consumers")
- Concrete job-to-be-done (not vague "productivity")
- Clear feasibility thesis tied to capability shifts
- Contrarian insights vs obvious plays

---

## Usage Example

**Phase 1 Output Example:**
"Shift: Context windows enable 'whole repository reasoning'
- Advantaged: Solo developers, small teams lacking institutional knowledge
- Disadvantaged: Traditional IDE vendors, documentation tools
- Friction: Trust in AI-generated insights, deployment complexity
- Synthetic worker: Repository archaeologist that reads entire codebase"

**Phase 2 Output Example:**
"Idea: Legal contract variation analyzer
- Customer: Mid-market legal departments
- Job-to-be-done: Identify deviations from standard clauses across 1000s of contracts
- Feasibility: Large context windows + function calling for clause extraction
- Why now: Previous 4K limit couldn't hold full contract + comparison set"

**Phase 3 Output Example:**
"#1 - Repository-aware code review agent
- Core bet: Engineers will pay $50/month for AI that understands full codebase context
- Test: Does quality improve vs Claude/GPT without repo context?
- MVP: VS Code extension + webhook for PR reviews in specific language/framework"

---

## Recommended Improvements

### Potential Enhancements
1. **Trend Integration**: Pull live data on AI capability benchmarks and pricing
2. **Competitive Intelligence**: Automatically scan for existing solutions
3. **Market Sizing**: Add TAM/SAM/SOM estimates for each opportunity
4. **Wardley Mapping**: Visualize value chain evolution for each idea
5. **Risk Assessment**: Systematic analysis of technical, market, regulatory risks
6. **Team Fit Analysis**: Match opportunities to specific founder strengths
7. **Playbook Library**: Catalog of go-to-market strategies by opportunity type

### Questions for Discussion
- Should this integrate real-time funding data (Crunchbase, AngelList)?
- Add "acquihire potential" as scoring dimension for strategics?
- Include regulatory landscape analysis for each opportunity?
- Support industry-specific variants (healthcare, fintech, legal, etc.)?
- Add "founder-market fit" questionnaire to personalize rankings?
- Create "signal tracking" to monitor when opportunities become obvious?
- Include "moat roadmap" showing how to build defensibility over time?
- Add collaboration mode for team ideation sessions?
